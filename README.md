<p>
  <a href="https://www.linkedin.com/in/darren-f%C3%BCrst-789049254/" target="_blank" rel="nofollow noreferrer" style="text-decoration:none; display:inline-flex; align-items:center; margin-right:10px;">
    <img src="https://i.sstatic.net/gVE0j.png" alt="LinkedIn" width="20" height="20" style="margin-right:5px;"> LinkedIn
  </a>
  <a href="https://scholar.google.com/citations?user=kjRyau4AAAAJ" target="_blank" rel="nofollow noreferrer" style="display:inline-flex; align-items:center;">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Google_Scholar_logo.svg/1024px-Google_Scholar_logo.svg.png" alt="Google Scholar" height="20">Google Scholar
  </a>
</p>



# <p align="center"> <img src="https://64.media.tumblr.com/9b440b905142f3aaeb9bdb43b1b8b97f/tumblr_ov9qtiN8x01u25kiio1_1280.gif" width="60"> <a href="https://cockneyrhymingslang.co.uk/slang/scooby_doo/">Just a bloke who hasn't a Scooby</a> <img src="https://64.media.tumblr.com/9b440b905142f3aaeb9bdb43b1b8b97f/tumblr_ov9qtiN8x01u25kiio1_1280.gif" width="60"> </p>



## Projects

<p align="center">
  ðŸš¨ <b>Check out <a href="https://alarmiator.de">ALARMiator</a></b> 
</p>
<p align="center">
  <a href="https://alarmiator.de" target="_blank">
    <img src="https://i0.wp.com/alarmiator.de/wp-content/uploads/2022/12/cropped-cropped-logo_front-e1670425833857-1.png?fit=210%2C49&ssl=1" alt="ALARMiator Logo" width="210" />
  </a>
</p>

----

##  ðŸ“š Latest Publications  ðŸ“š 
[![ORCID](https://img.shields.io/badge/ORCID-0000--0002--1825--0097-brightgreen?logo=orcid&logoColor=white)](https://orcid.org/0009-0006-3607-349X)

<!-- ORCID-PUBS:START -->
- 2025: *Practical Acoustic Eavesdropping On Typed Passphrases* â€” Darren FÃ¼rst, Andreas AÃŸmuth [DOI](https://pub.orcid.org/v3.0/0009-0006-3607-349X/work/180723448)
  <details>
    <summary>Abstract</summary>

    Cloud services have become an essential infrastructure for enterprises and individuals. Access to these cloud services is typically governed by Identity and Access Management systems, where user authentication often relies on passwords. While best practices dictate the implementation of multi-factor authentication, it's a reality that many such users remain solely protected by passwords. This reliance on passwords creates a significant vulnerability, as these credentials can be compromised through various means, including side-channel attacks. This paper exploits keyboard acoustic emanations to infer typed natural language passphrases via unsupervised learning, necessitating no previous training data. Whilst this work focuses on short passphrases, it is also applicable to longer messages, such as confidential emails, where the margin for error is much greater, than with passphrases, making the attack even more effective in such a setting. Unlike traditional attacks that require physical access to the target device, acoustic side-channel attacks can be executed within the vicinity, without the user's knowledge, offering a worthwhile avenue for malicious actors. Our findings replicate and extend previous work, confirming that cross-correlation audio preprocessing outperforms methods like mel-frequency-cepstral coefficients and fast-fourier transforms in keystroke clustering. Moreover, we show that partial passphrase recovery through clustering and a dictionary attack can enable faster than brute-force attacks, further emphasizing the risks posed by this attack vector.
  </details>
- 2024: *Question: How do Large Language Models perform on the Question Answering tasks? Answer:* â€” Kevin Fischer, Darren FÃ¼rst, Jakob Lindner, Sebastian Steindl, Ulrich SchÃ¤fer [DOI](https://pub.orcid.org/v3.0/0009-0006-3607-349X/work/177903007)
  <details>
    <summary>Abstract</summary>

    Large Language Models (LLMs) have been showing promising results for various NLP-tasks without the explicit need to be trained for these tasks by using few-shot or zero-shot prompting techniques. A common NLP-task is question-answering (QA). In this study, we propose a comprehensive performance comparison between smaller fine-tuned models and out-of-the-box instruction-following LLMs on the Stanford Question Answering Dataset 2.0 (SQuAD2), specifically when using a single-inference prompting technique. Since the dataset contains unanswerable questions, previous work used a double inference method. We propose a prompting style which aims to elicit the same ability without the need for double inference, saving compute time and resources. Furthermore, we investigate their generalization capabilities by comparing their performance on similar but different QA datasets, without fine-tuning neither model, emulating real-world uses where the context and questions asked may differ from the original training distribution, for example swapping Wikipedia for news articles.  Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test and even outperform the fine-tuned models on 3 of the 5 tested QA datasets.
  </details>
<!-- ORCID-PUBS:END -->

---

<picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/N0tAScooby/N0tAScooby/output/github-contribution-grid-snake-dark.svg">
  <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/N0tAScooby/N0tAScooby/output/github-contribution-grid-snake.svg">
  <img alt="github contribution grid snake animation" src="https://raw.githubusercontent.com/N0tAScooby/N0tAScooby/output/github-contribution-grid-snake.svg">
</picture>

